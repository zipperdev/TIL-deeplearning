import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist


"""
ë”¥ëŸ¬ë‹ = ì¢…ë‹¨ê°„ ê¸°ê³„í•™ìŠµ (e2e machine learning)
ì¢…ë‹¨ê°„ ê¸°ê³„í•™ìŠµì€ ì¸ê°„ì´ ì²˜ìŒë¶€í„° ëê¹Œì§€ ê°œì…í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸

ê¸°ê³„í•™ìŠµì€ í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ë¡œ ë‚˜ëˆ  í•™ìŠµê³¼ ì‹¤í—˜ ìˆ˜í–‰
ë²”ìš© ëŠ¥ë ¥ì€ ì ‘í•˜ì§€ ëª»í•œ ë°ì´í„°ë¥¼ í’€ì–´ë‚´ëŠ” ëŠ¥ë ¥
ë²”ìš© ëŠ¥ë ¥ì„ ì œëŒ€ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ í›ˆë ¨/ì‹œí—˜ ë°ì´í„°ë¡œ ìª¼ê°¬

ì˜¤ë²„í”¼íŒ…ì€ íŠ¹ì • ë°ì´í„°ì…‹ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ëœ ìƒíƒœë¥¼ ì˜ë¯¸
ê¸°ê³„í•™ìŠµ ì‹œ ì˜¤ë²„í”¼íŒ…ì„ í”¼í•´ì•¼í•¨
"""


# ì†ì‹¤ í•¨ìˆ˜
# ===

"""
ì†ì‹¤ í•¨ìˆ˜ (loss function)ëŠ” ì‹ ê²½ë§ í•™ìŠµì—ì„œ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ ì„±ëŠ¥ì˜ ë‚˜ì¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ
"""


"""
ì˜¤ì°¨ì œê³±í•© (Sum of Squares for Error, SSE)

E = 1/2 âˆ‘(k) (y_k - t_k)^2
këŠ” ë°ì´í„°ì˜ ì°¨ì› ìˆ˜, y_këŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥, t_këŠ” ì •ë‹µ ë ˆì´ë¸”
"""

def sum_squares_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)


"""
êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ (Cross Entropy Error, CEE)

E = -âˆ‘(k) t_k log y_k
"""

def cross_entropy_error(y, t):
    delta = 1e-7  # np.log(0) = -inf ë°©ì§€
    return -np.sum(t * np.log(y + delta))


# 2ê°€ ì •ë‹µ
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])

# 2ì¼ í™•ë¥ ì´ ë†’ë‹¤ê³  ì¶”ì •
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
print(sum_squares_error(y, t))
print(cross_entropy_error(y, t))

# 7ì¼ í™•ë¥ ì´ ë†’ë‹¤ê³  ì¶”ì •
y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])
print(sum_squares_error(y, t))
print(cross_entropy_error(y, t))


# ë¯¸ë‹ˆë°°ì¹˜ì™€ ì†ì‹¤ í•¨ìˆ˜

"""
ë°°ì¹˜ëŠ” ì—¬ëŸ¬ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë¯€ë¡œ ë‹¨ì¼ ë°°ì¹˜ì— ëŒ€í•´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ì•¼ë¨

E = -1/N âˆ‘(n) âˆ‘(k) t_nk log y_nk
ìœ„ëŠ” ë°ì´í„° Nê°œ ë°°ì¹˜ì— ëŒ€í•œ í‰ê·  êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

ì´ë•Œ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ë¯€ë¡œ, ì¼ë¶€ë§Œ ê³¨ë¼ í•™ìŠµí•¨
ì´ ì¼ë¶€ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ (mini-batch), ì´ëŸ¬í•œ í•™ìŠµ ë°©ë²•ì„ ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì´ë¼ê³  í•¨
"""

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(* np.log(y[np.arange(batch_size), t] + 1e-7))


"""
ì™œ ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ì‚¬ìš©í• ê¹Œ?

ì‹ ê²½ë§ í•™ìŠµì—ì„œëŠ” ë¯¸ë¶„ì„ ì‚¬ìš©í•¨
ì •í™•ë„ëŠ” ë§¤ê°œë³€ìˆ˜ì— ì•½ê°„ì˜ ë³€í™”ë¥¼ ì£¼ë©´ ë¶ˆì—°ì†ì ìœ¼ë¡œ ë¯¸ì„¸í•˜ê²Œ ë³€í•¨
ë”°ë¼ì„œ ì •í™•ë„ë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í•˜ë©´ ë¯¸ë¶„ì˜ ê°’ì´ ëŒ€ë¶€ë¶„ 0ì— ê°€ê¹ê¸° ë•Œë¬¸ì— ë°œì „í•  ìˆ˜ ì—†ìŒ

ë¹„ìŠ·í•œ ì´ìœ ë¡œ í™œì„±í™” í•¨ìˆ˜ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•¨
"""


# ë¯¸ë¶„
# ===

# ìˆ˜ì¹˜ ë¯¸ë¶„

"""
í•´ì„ì  ë¯¸ë¶„ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ìŒ
í•˜ì§€ë§Œ hê°€ ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ë¡œ í‘œí˜„ì´ ë¶€ì •í™•í•  ìˆ˜ ìˆê³ , yì˜ ì°¨ì— ì˜¤ì°¨ê°€ ìƒê¸°ê¸°ë„ í•¨ 
"""

def numerical_diff(f, x):
    h = 1e-50
    return (f(x+h) - f(x)) / h

"""
ê·¸ëŸ¬ë¯€ë¡œ ì»´í“¨í„°ì—ì„œ ë¯¸ë¶„ì€ ê°œì„ ì´ í•„ìš”í•¨

hë¥¼ 10^-4 ì •ë„ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì ë‹¹í•¨
ì´ì œ hì˜ ê°’ì´ 0ì— ëœ ê·¼ì ‘í•˜ë¯€ë¡œ í•¨ìˆ˜ fì˜ ì°¨ë¶„ì— ì˜¤ì°¨ê°€ ë°œìƒí•¨
ë”°ë¼ì„œ (x + h)ì™€ (x - h)ì¼ ë•Œì˜ í•¨ìˆ˜ fì˜ ì°¨ë¶„, ì¤‘ì‹¬ ì°¨ë¶„ì„ ì‚¬ìš©í•¨
((x + h)ì™€ xì¼ ë•Œì˜ ì°¨ë¶„ì„ ì „ë°© ì°¨ë¶„ì´ë¼ í•¨)

ìœ„ ê°œì„ ì„ ì ìš©í•œ ë¯¸ë¶„ì„ ìˆ˜ì¹˜ ë¯¸ë¶„ì´ë¼ í•¨  
"""

del numerical_diff
def numerical_diff(f, x):
    h = 1e-4
    return (f(x+h) - f(x-h)) / (2 * h)


function_1 = lambda x: 0.01 * x ** 2 + 0.1 * x
x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")

def tangent_line(f, x):
    d = numerical_diff(f, x)
    y = f(x) - d*x
    return lambda t: d*t + y

tf = tangent_line(function_1, 5)
y2 = tf(x)
plt.plot(x, y)
plt.plot(x, y2, linestyle="--")
plt.show()


# í¸ë¯¸ë¶„

"""
í¸ë¯¸ë¶„ = ë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„

f(x_0, x_1) = x_0^2 + x_1^2, x_0 = 3, x_1 = 4ì¼ ë•Œ, x_0ì— ëŒ€í•œ í¸ë¯¸ë¶„ Î´f/Î´x_0ë¥¼ êµ¬í•´ë³´ì.
"""

function_2 = lambda x: x[0]**2 + x[1]**2

def function_tmp1(x0):
    return x0**2.0 + 4.0**2.0

print(numerical_diff(function_tmp1, 3))

"""
ìœ„ì™€ ê°™ì´ ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, ë³€ìˆ˜ë¥¼ ê³ ì •í•´ì„œ í’€ ìˆ˜ ìˆìŒ
"""


# ê¸°ìš¸ê¸°

"""
ìœ„ì—ì„  ë³€ìˆ˜ê°€ í•˜ë‚˜ë¡œ ê³ ì •ëœ í¸ë¯¸ë¶„ì„ í’ˆ
x_0, x_1ì˜ í¸ë¯¸ë¶„ì„ ë™ì‹œì— ê³„ì‚°í•˜ëŠ” ê²½ìš°ë„ ìˆìŒ
ì´ë•Œ (Î´f/Î´x_0, Î´f/Î´x_1)ì²˜ëŸ¼ ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ ë°±í„°ë¡œ ì •ë¦¬í•œ ê²ƒì„ ê¸°ìš¸ê¸° (gradient)ë¼ê³  í•¨
"""

def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)

    for idx in range(x.size):
        tmp = x[idx]

        # f(x + h)ì˜ ê°’
        x[idx] = tmp + h
        fxh1 = f(x)

        # f(x - h)ì˜ ê°’
        x[idx] = tmp - h
        fxh2 = f(x)

        # ìˆ˜ì¹˜ ë¯¸ë¶„
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp

    return grad

# f(x_0, x_1) = x_0^2 + x_1^2, (x_0, x_1) = (3, 4)ì¼ ë•Œì˜ ê¸°ìš¸ê¸°
print(numerical_gradient(function_2, np.array([3.0, 4.0])))


"""
ê¸°ìš¸ê¸° => ê° ì§€ì ì—ì„œ í•¨ìˆ˜ì˜ ê°’ì„ ë‚®ì¶”ëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ì§€í‘œ
ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ë³µì¡í•œ í•¨ìˆ˜ì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ê³³ì´ ìµœì†Ÿê°’ì´ë€ ë³´ì¥ì€ ì—†ìŒ
"""


# ê²½ì‚¬ë²•
# ===


"""
ê²½ì‚¬ë²• (gradient method) = í˜„ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œí¼ ì´ë™ì„ ë°˜ë³µí•˜ë©° ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜í•´ í•¨ìˆ˜ì˜ ê°’ì„ ì ì°¨ ì¤„ì´ëŠ” ê²ƒ

ìµœì†Ÿê°’ì„ ì°¾ëŠ”ë‹¤ë©´ ê²½ì‚¬ í•˜ê°•ë²• (gradient descent method), ìµœëŒ“ê°’ì„ ì°¾ëŠ”ë‹¤ë©´ ê²½ì‚¬ ìƒìŠ¹ë²• (gradient ascent method)ì´ë¼ê³  í•¨
ì‹ ê²½ë§ í•™ìŠµì—ì„œ ê²½ì‚¬ë²•ì„ ë§ì´ ì‚¬ìš©í•¨

x_0 = x_0 - ğ¶ Î´f/Î´x_0
x_1 = x_1 - ğ¶ Î´f/Î´x_1

ğ¶ (ì—íƒ€) = ê°±ì‹ í•˜ëŠ” ì–‘
ì‹ ê²½ë§ì—ì„œëŠ” ğ¶ë¥¼ í•™ìŠµë¥  (learning rate)ë¼ê³  í•¨ 
í•™ìŠµë¥ ê³¼ ê°™ì€ ë§¤ê°œë³€ìˆ˜ëŠ” í•™ìŠµí•  ìˆ˜ ì—†ì–´ ì§ì ‘ ì§€ì •í•˜ë¯€ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° (hyper parameter)ë¼ê³  ë¶€ë¦„
"""

def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x


def gradient_descent_history(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append(x.copy())

        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)

x, x_history = gradient_descent_history(function_2, np.array([-3.0, 4.0]), lr=0.1, step_num=40)

plt.plot([-5, 5], [0, 0], "--b")
plt.plot([0, 0], [-5, 5], "--b")
plt.plot(x_history[:, 0], x_history[:, 1], "o")

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("x0")
plt.ylabel("x1")
plt.show()
